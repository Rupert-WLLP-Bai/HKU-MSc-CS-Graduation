\chapter{Preliminary}

\section{Problem Definition}

Click-through rate (CTR) prediction is a binary classification task. The conventional CTR prediction dataset $\mathcal{D}$ consists of $N$ samples, each represented as a pair $(\mathbf{x}_i, y_i)$, where the label $y_i \in \{0, 1\}$ indicates the user’s click behavior (1 for click, 0 for no click). The input instance is denoted by $\mathbf{x}_i = \{x_{i,1}, x_{i,2}, \cdots, x_{i,m}\}$, where $m$ represents the number of feature fields, and each feature $x_{i,m}$ is a high-dimensional one-hot vector.

In traditional CTR scenarios, a $D$-dimensional embedding layer $\mathcal{E} = \{E_1, E_2, \cdots, E_m\}$ maps the high-dimensional sparse vector $\mathbf{x}_i$ into a low-dimensional dense embedding $\mathbf{e}_i = \{e_{i,1}, e_{i,2}, \cdots, e_{i,m}\}$, where $e_{i,m} \in \mathbb{R}^{D}$, which is more suitable for learning in deep models. The CTR prediction model $f_{\text{CTR}}$ then estimates the click probability $y_i$ based on these embeddings, formulated as:
\[
y_i = f_{\text{CTR}}(e_{i,1}, e_{i,2}, \cdots, e_{i,m}).
\]

\textbf{Text-enhanced CTR prediction:} With the advent of Pretrained Language Models (PLMs), some studies have attempted to incorporate textual information into CTR prediction. Let $t_i$ denote the textual information associated with the sample $(\mathbf{x}_i, y_i)$, which may include the item title, description, and reviews. This textual information is typically encoded by a PLM, formulated as:
\[
\mathbf{t}_i = f_{\text{PLM}}(t_i), \quad \mathbf{t}_i \in \mathbb{R}^{D_{\text{PLM}}},
\]
where $D_{\text{PLM}}$ denotes the dimension of the PLM embedding. The semantic embedding $\mathbf{t}_i$ is then integrated into the CTR model to enhance prediction performance:
\[
y_i = f_{\text{CTR}}(f_{\text{text}}(\mathbf{t}_i), e_{i,1}, e_{i,2}, \cdots, e_{i,m}),
\]
where $f_{\text{text}}$ is a fully connected network that aligns the embedding size to facilitate feature interactions.

Due to the rich information in $t_i$, the encoded semantic embedding $\mathbf{t}_i$ often captures multiple latent aspects, leading to an entangled representation. In this study, we focus on disentangling the entangled semantic embedding $\mathbf{t}_i$ into distinct, meaningful embeddings to enhance interpretability and effectiveness.

\section{Topic Model}

To perform the disentangling task, we employ topic modeling to capture distinct semantic aspects from textual information. A topic model is a statistical approach for discovering hidden semantic structures in a collection of documents. In this study, we treat the textual information of all items as a document corpus, denoted as $\mathcal{T} = \{t_j \mid j \in [1, 2, \cdots, J]\}$, where $J$ is the total number of items. Each document $t_j$ is represented as a Bag-of-Words (BoW) vector $\mathbf{b}_j \in \mathbb{R}^{V}$, where $V$ is the vocabulary size.

The topic model assumes that each document is generated from a mixture of topics (document’s topic distribution), where each topic is characterized by a distribution over words (topic-word distribution). ETM factorizes the topic-word distribution into a topic embedding matrix $\alpha \in \mathbb{R}^{K \times D_{\text{TM}}}$ and a word embedding matrix $\rho \in \mathbb{R}^{V \times D_{\text{TM}}}$, where $K$ is the number of topics and $D_{\text{TM}}$ is the dimensionality of the embeddings. The topic-word distribution matrix $\beta \in \mathbb{R}^{K \times V}$ is computed as:
\[
\beta = \text{Softmax}(\alpha \rho^\top),
\]
where $\beta_{k,v}$ represents the probability of word $v$ given topic $k$.

Let $b_{j,v}$ denote the $v$-th word in document $t_j$. The generative process of $t_j$ follows:
\begin{enumerate}
    \item Draw topic proportions $\theta_j \sim \mathcal{LN}(0, I)$.
    \item For each word $b_{j,v}$ in document $t_j$:
    \begin{enumerate}
        \item Draw a topic assignment $z_{j,v} \sim \text{Cat}(\theta_j)$.
        \item Draw the word $b_{j,v} \sim \text{Cat}(\beta_{z_{j,v}})$.
    \end{enumerate}
\end{enumerate}

Here, $\mathcal{LN}(0, I)$ represents the logistic-normal distribution, which maps a standard Gaussian random variable into the probability simplex, and $\text{Cat}(\cdot)$ denotes the categorical distribution.
